{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "underlying-telling",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Requirements:\n",
    "# !pip install rtdl\n",
    "# !pip install libzero==0.0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "scheduled-leone",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Dict\n",
    "\n",
    "import numpy as np\n",
    "import rtdl\n",
    "import scipy.special\n",
    "import sklearn.datasets\n",
    "import sklearn.metrics\n",
    "import sklearn.model_selection\n",
    "import sklearn.preprocessing\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import zero\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bigger-frame",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "123456"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cpu')\n",
    "# Docs: https://yura52.github.io/zero/0.0.4/reference/api/zero.improve_reproducibility.html\n",
    "zero.improve_reproducibility(seed=123456)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acquired-contractor",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "addressed-congo",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/kdd_processed.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "macro-review",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(\"Unnamed: 0\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "genuine-stomach",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.values\n",
    "\n",
    "target = df[\"target\"].values\n",
    "targetValues = list(set(target))\n",
    "targetMapping = dict()\n",
    "\n",
    "\n",
    "for i in range(len(targetValues)):\n",
    "    targetMapping[targetValues[i]] = i\n",
    "\n",
    "result = []\n",
    "for t in target:\n",
    "    result.append(targetMapping[t])\n",
    "\n",
    "dfFormat = {\"target\": pd.DataFrame(result).values.reshape(df.shape[0]), \"data\": df.drop(\"target\", axis=1), \"frame\": None, \"DESCR\": \"Todo\", \"feature_names\": []}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fabulous-mortality",
   "metadata": {},
   "outputs": [],
   "source": [
    "def toY(_data):\n",
    "    if type(_data) == np.ndarray:\n",
    "        return _data\n",
    "    else:\n",
    "        return _data.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "streaming-variety",
   "metadata": {},
   "outputs": [],
   "source": [
    "task_type = 'multiclass'\n",
    "\n",
    "assert task_type in ['binclass', 'multiclass', 'regression']\n",
    "\n",
    "X_all = dfFormat['data'].astype('float32')\n",
    "y_all = dfFormat['target'].astype('float32' if task_type == 'regression' else 'int64')\n",
    "if task_type != 'regression':\n",
    "    y_all = sklearn.preprocessing.LabelEncoder().fit_transform(y_all).astype('int64')\n",
    "n_classes = int(max(y_all)) + 1 if task_type == 'multiclass' else None\n",
    "\n",
    "oldX = {}\n",
    "y = {}\n",
    "oldX['train'], oldX['test'], y['train'], y['test'] = sklearn.model_selection.train_test_split(\n",
    "    X_all, y_all, train_size=0.8\n",
    ")\n",
    "oldX['train'], oldX['val'], y['train'], y['val'] = sklearn.model_selection.train_test_split(\n",
    "    oldX['train'], y['train'], train_size=0.8\n",
    ")\n",
    "\n",
    "# not the best way to preprocess features, but enough for the demonstration\n",
    "preprocess = sklearn.preprocessing.StandardScaler().fit(oldX['train'])\n",
    "X = {\n",
    "    # k: torch.tensor(v, device=device)\n",
    "    k: torch.tensor(preprocess.fit_transform(v), device=device)\n",
    "    for k, v in oldX.items()\n",
    "}\n",
    "# y = {k: torch.tensor(np.array(v), device=device) for k, v in y.items()}\n",
    "y = {k: torch.tensor(toY(v), device=device) for k, v in y.items()}\n",
    "\n",
    "# !!! CRUCIAL for neural networks when solving regression problems !!!\n",
    "if task_type == 'regression':\n",
    "    y_mean = y['train'].mean().item()\n",
    "    y_std = y['train'].std().item()\n",
    "    y = {k: (v - y_mean) / y_std for k, v in y.items()}\n",
    "else:\n",
    "    y_std = y_mean = None\n",
    "\n",
    "if task_type != 'multiclass':\n",
    "    y = {k: v.float() for k, v in y.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decent-idaho",
   "metadata": {},
   "source": [
    "### Model\n",
    "Carefully read the comments and uncomment the code for the model you want to test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "greatest-impact",
   "metadata": {},
   "outputs": [],
   "source": [
    "def createModel():\n",
    "    d_out = n_classes or 1\n",
    "\n",
    "\n",
    "    first_layer = 4\n",
    "#     _model = rtdl.MLP.make_baseline(\n",
    "#         d_in=X_all.shape[1],\n",
    "#     #     d_layers=[first_layer, 256, 128],\n",
    "#         d_layers=[first_layer, 8, first_layer],\n",
    "#         dropout=0.1,\n",
    "#         d_out=d_out,\n",
    "#         # seed=42\n",
    "#     )\n",
    "    lr = 0.001\n",
    "    weight_decay = 0.0\n",
    "\n",
    "    # model = rtdl.ResNet.make_baseline(\n",
    "    #     d_in=X_all.shape[1],\n",
    "    #     d_main=128,\n",
    "    #     d_intermidiate=256,\n",
    "    #     dropout_first=0.2,\n",
    "    #     dropout_second=0.0,\n",
    "    #     n_blocks=2,\n",
    "    #     d_out=d_out,\n",
    "    # )\n",
    "    # lr = 0.001\n",
    "    # weight_decay = 0.0\n",
    "\n",
    "    _model = rtdl.FTTransformer.make_default(\n",
    "        n_num_features=X_all.shape[1],\n",
    "        cat_cardinalities=None,\n",
    "        n_blocks=1,\n",
    "        last_layer_query_idx=[-1],  # it makes the model faster and does NOT affect its output\n",
    "        d_out=d_out,\n",
    "    )\n",
    "\n",
    "    # === ABOUT CATEGORICAL FEATURES ===\n",
    "    # IF you use MLP, ResNet or any other simple feed-forward model (NOT transformer-based model)\n",
    "    # AND there are categorical features\n",
    "    # THEN you have to implement a wrapper that handles categorical features.\n",
    "    # The example below demonstrates how it can be achieved using rtdl.CategoricalFeatureTokenizer.\n",
    "    # ==================================\n",
    "    # 1. When you have both numerical and categorical features, you should prepare you data like this:\n",
    "    #    (X_num<float32>, X_cat<int64>) instead of X<float32>\n",
    "    #    Each column in X_cat should contain values within the range from 0 to <(the number of unique values in column) - 1>;\n",
    "    #    use sklean.preprocessing.OrdinalEncoder to achieve this;\n",
    "    # 2. Prepare a list of so called \"cardinalities\":\n",
    "    #    cardinalities[i] = <the number of unique values of the i-th categorical feature>\n",
    "    # 3. See the commented example below and adapt it for your needs.\n",
    "    #\n",
    "    # class Model(nn.Module):\n",
    "    #     def __init__(\n",
    "    #         self,\n",
    "    #         n_num_features: int,\n",
    "    #         cat_tokenizer: rtdl.CategoricalFeatureTokenizer,\n",
    "    #         mlp_kwargs: Dict[str, Any],\n",
    "    #     ):\n",
    "    #         super().__init__()\n",
    "    #         self.cat_tokenizer = cat_tokenizer\n",
    "    #         self.model = rtdl.MLP.make_baseline(\n",
    "    #             d_in=n_num_features + cat_tokenizer.n_tokens * cat_tokenizer.d_token,\n",
    "    #             **mlp_kwargs,\n",
    "    #         )\n",
    "    #\n",
    "    #     def forward(self, x_num, x_cat):\n",
    "    #         return self.model(\n",
    "    #             torch.cat([x_num, self.cat_tokenizer(x_cat).flatten(1, -1)], dim=1)\n",
    "    #         )\n",
    "    #\n",
    "    # model = Model(\n",
    "    #     # `None` means \"Do not transform numerical features\"\n",
    "    #     # `d_token` is the size of embedding for ONE categorical feature\n",
    "    #     X_num_all.shape[1],\n",
    "    #     rtdl.CategoricalFeatureTokenizer(cardinalities, d_token, True, 'uniform'),\n",
    "    #     mlp_kwargs,\n",
    "    # )\n",
    "    # Then the model should be used as `model(x_num, x_cat)` instead of of `model(x)`.\n",
    "\n",
    "    _model.to(device)\n",
    "    optimizer = (\n",
    "        _model.make_default_optimizer()\n",
    "        if isinstance(_model, rtdl.FTTransformer)\n",
    "    #     else torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "        else torch.optim.Adam(_model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    )\n",
    "    loss_fn = (\n",
    "        F.binary_cross_entropy_with_logits\n",
    "        if task_type == 'binclass'\n",
    "        else F.cross_entropy\n",
    "        if task_type == 'multiclass'\n",
    "        else F.mse_loss\n",
    "    )\n",
    "    return _model, optimizer, loss_fn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "convinced-disposal",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, optimizer, loss_fn = createModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lonely-monday",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "inclusive-tattoo",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test score before training: 0.0049\n"
     ]
    }
   ],
   "source": [
    "def apply_model(x_num, x_cat=None, model=None):\n",
    "    if isinstance(model, rtdl.FTTransformer):\n",
    "        return model(x_num, x_cat)\n",
    "    elif isinstance(model, (rtdl.MLP, rtdl.ResNet)):\n",
    "        assert x_cat is None\n",
    "        return model(x_num)\n",
    "    else:\n",
    "        raise NotImplementedError(\n",
    "            f'Looks like you are using a custom model: {type(model)}.'\n",
    "            ' Then you have to implement this branch first.'\n",
    "        )\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(part, model):\n",
    "    model.eval()\n",
    "    prediction = []\n",
    "    for batch in zero.iter_batches(X[part], 1024):\n",
    "        prediction.append(apply_model(batch,model=model))\n",
    "    prediction = torch.cat(prediction).squeeze(1).cpu().numpy()\n",
    "    target = y[part].cpu().numpy()\n",
    "\n",
    "    if task_type == 'binclass':\n",
    "        prediction = np.round(scipy.special.expit(prediction))\n",
    "        score = sklearn.metrics.accuracy_score(target, prediction)\n",
    "    elif task_type == 'multiclass':\n",
    "        prediction = prediction.argmax(1)\n",
    "        score = sklearn.metrics.accuracy_score(target, prediction)\n",
    "    else:\n",
    "        assert task_type == 'regression'\n",
    "        score = sklearn.metrics.mean_squared_error(target, prediction) ** 0.5 * y_std\n",
    "    return score\n",
    "\n",
    "\n",
    "# Create a dataloader for batches of indices\n",
    "# Docs: https://yura52.github.io/zero/reference/api/zero.data.IndexLoader.html\n",
    "batch_size = 1 ##256\n",
    "train_loader = zero.data.IndexLoader(len(X['train']), batch_size, device=device)\n",
    "\n",
    "# Create a progress tracker for early stopping\n",
    "# Docs: https://yura52.github.io/zero/reference/api/zero.ProgressTracker.html\n",
    "progress = zero.ProgressTracker(patience=100)\n",
    "\n",
    "print(f'Test score before training: {evaluate(\"test\",model):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "annual-liver",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def plotLosses(_losses, title=\"this is a graph\",log=True):\n",
    "    for key in _losses:\n",
    "        if log:\n",
    "            plt.plot([np.log(x) for x in _losses[key]], label=key)\n",
    "        else:\n",
    "            plt.plot(_losses[key], label=key)\n",
    "    plt.xlabel(\"epoch\")\n",
    "    plt.ylabel(\"loss\")\n",
    "    plt.legend()\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "personalized-house",
   "metadata": {},
   "outputs": [],
   "source": [
    "def learnThat(_model, _optimizer, _loss_fn, _evaluate, _progress,_X, _y, _epochs, _batch_size,_train_loader, _relational_batch, _old_X, print_mode):\n",
    "\n",
    "    report_frequency = len(X['train']) // _batch_size // 5\n",
    "    losses = dict()\n",
    "    losses['val']  = []\n",
    "    losses['test'] = []\n",
    "    for epoch in range(1, _epochs + 1):\n",
    "        for iteration, batch_idx in enumerate(train_loader):\n",
    "            _model.train()\n",
    "            _optimizer.zero_grad()\n",
    "            x_batch = _X['train'][batch_idx]\n",
    "            y_batch = _y['train'][batch_idx]\n",
    "            loss = _loss_fn(apply_model(x_batch,model=_model).squeeze(1), y_batch)\n",
    "            loss.backward()\n",
    "\n",
    "            factors = dict()\n",
    "\n",
    "            ## Modify gradients\n",
    "            if _relational_batch:\n",
    "                for name, param in model.named_parameters():\n",
    "                    if name==\"feature_tokenizer.num_tokenizer.weight\":\n",
    "                    # if name==\"blocks.0.linear.weight\":                    \n",
    "                        column_count = df.shape[1] - 1\n",
    "                        \n",
    "                        factorSize = tuple([param.grad.shape[0],param.grad.shape[1]])\n",
    "                        \n",
    "                        \n",
    "                        factors = torch.ones(param.grad.shape)\n",
    "                        for i in range(column_count):\n",
    "                            column = oldX['train'].columns[i]\n",
    "                            if True:# not column in oldNames:\n",
    "                                idx = oldX['train'][iteration * batch_size:(iteration+1) * batch_size].columns[i]\n",
    "                                realCount = oldX['train'][iteration * batch_size:(iteration+1) * batch_size][idx].sum()\n",
    "                                if realCount > 0:\n",
    "                                    factors[i] = (batch_size / (1.0 * realCount)) * factors[i]\n",
    "                                else:\n",
    "                                    ()\n",
    "                        ## TO CHECK !\n",
    "                        param.grad = torch.mul(param.grad, factors)\n",
    "\n",
    "            optimizer.step()\n",
    "            if iteration % report_frequency == 0:\n",
    "                batch = \"batch\"\n",
    "                if relational_batch:\n",
    "                    batch= \"relational-batch\"\n",
    "                if print_mode:\n",
    "                    print(f'(epoch) {epoch} ({batch}) {iteration} (loss) {loss.item():.4f}')\n",
    "\n",
    "\n",
    "        losses['val'].append(float(loss_fn(apply_model(X['val'],model=_model).squeeze(1), y['val'])))\n",
    "        losses['test'].append(float(loss_fn(apply_model(X['test'],model=_model).squeeze(1), y['test'])))\n",
    "\n",
    "\n",
    "        val_score  = _evaluate('val',_model)\n",
    "        test_score = _evaluate('test',_model)\n",
    "        _progress.update((-1 if task_type == 'regression' else 1) * val_score)\n",
    "        if _progress.fail:\n",
    "            break\n",
    "            \n",
    "        if print_mode:\n",
    "            print(f'Epoch {epoch:03d} | Validation score: {val_score:.4f} | Test score: {test_score:.4f}', end='')\n",
    "            if _progress.success:\n",
    "                print(' <<< BEST VALIDATION EPOCH', end='')\n",
    "            print()\n",
    "        \n",
    "    if print_mode:\n",
    "        plotLosses(losses, \"relational batch ? \" + str(_relational_batch))\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "numerous-correction",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "plastic-buffalo",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = dict()\n",
    "results[\"rb\"] = []\n",
    "results[\"norb\"] = []\n",
    "k = 5\n",
    "for i in range(k):\n",
    "    print(i)\n",
    "    model, optimizer, loss_fn = createModel()\n",
    "    relational_batch = True\n",
    "    losses = learnThat(\n",
    "        _model    =model, \n",
    "        _optimizer=optimizer, \n",
    "        _loss_fn  =loss_fn, \n",
    "        _evaluate =evaluate, \n",
    "        _progress =progress,\n",
    "        _X=X, \n",
    "        _y=y, \n",
    "        _epochs      =epochs, \n",
    "        _batch_size  =batch_size,\n",
    "        _train_loader=train_loader, \n",
    "        _relational_batch=relational_batch, \n",
    "        _old_X=oldX,\n",
    "        print_mode=False)\n",
    "    results[\"rb\"].append(losses[\"test\"])\n",
    "    \n",
    "    model, optimizer, loss_fn = createModel()\n",
    "    relational_batch = False\n",
    "    losses = learnThat(\n",
    "        _model    =model, \n",
    "        _optimizer=optimizer, \n",
    "        _loss_fn  =loss_fn, \n",
    "        _evaluate =evaluate, \n",
    "        _progress =progress,\n",
    "        _X=X, \n",
    "        _y=y, \n",
    "        _epochs      =epochs, \n",
    "        _batch_size  =batch_size,\n",
    "        _train_loader=train_loader, \n",
    "        _relational_batch=relational_batch, \n",
    "        _old_X=oldX,\n",
    "        print_mode=False)\n",
    "    results[\"norb\"].append(losses[\"test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incomplete-consortium",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accessible-fabric",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "swedish-sandwich",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Random test data\n",
    "labels = [\"rb\", \"norb\"]\n",
    "colors = ['pink', 'lightblue']\n",
    "\n",
    "finalLosses = [[x[-1] for x in results[label]] for label in labels]\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(4, 4))\n",
    "\n",
    "# rectangular box plot\n",
    "bplot = ax.boxplot(finalLosses,\n",
    "                     vert=True,  # vertical box alignment\n",
    "                     patch_artist=True,  # fill with color\n",
    "                     labels=labels)  # will be used to label x-ticks\n",
    "# ax1.set_title('Rectangular box plot')\n",
    "\n",
    "for patch, color in zip(bplot['boxes'], colors):\n",
    "    patch.set_facecolor(color)\n",
    "\n",
    "# adding horizontal grid lines\n",
    "ax.yaxis.grid(True)\n",
    "# ax.set_xlabel('Three separate samples')\n",
    "ax.set_ylabel('Final Loss')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "amber-decimal",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
